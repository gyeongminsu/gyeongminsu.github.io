---
title : Diffusion Models Without Attention & SSM(State Space Model) ì™„ì „ ì •ë³µ 
categories : SSM, S4, HiPPO, Mamba, Diffusion, DDPM
tags : SSM, S4, HiPPO, Mamba, Diffusion, DDPM
date : 2024-05-18 00:00:00 +0900
pin : true
path : true
math : true
image : /assets/img/2024-05-18-Diffusion Models Without Attention&SSM/thumbnail.png
toc : true
layout : post
comments : true
---

# Diffusion Models Without Attention ë…¼ë¬¸ ë¦¬ë·°

[arxiv.org](https://arxiv.org/pdf/2311.18257)

# 0. Abstract

ìµœê·¼ ê³ í™”ì§ˆ(hi-fidelity) ì´ë¯¸ì§€ ìƒì„± ë¶„ì•¼ì˜ ë°œì „ ê³¼ì •ì—ì„œ Denoising Diffusion Probability Model(ì´í•˜ DDPM)ì€ ìƒì„± ëª¨ë¸ë¡œì„œ ë§¤ìš° ì¤‘ìš”í•œ Key playerë¡œ ë“±ì¥í•˜ê²Œ ë˜ì—ˆë‹¤.

ê·¸ëŸ¬ë‚˜ DDPMì„ ì´ìš©í•˜ì—¬ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì—ëŠ” ë†’ì€ Computational resourceê°€ í•„ìš”í•˜ë‹¤.

U-Netì´ë‚˜ Transformer ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„í• í•˜ì—¬ input dataë¡œ ì´ìš©í•˜ëŠ” imageì˜ íŒ¨ì¹˜í™”(patchifying)ê³¼ ê°™ì€ ë°©ë²•ì„ ì´ìš©í•´ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¹„ìš©ì„ ì¤„ì˜€ì§€ë§Œ, ì´ëŸ¬í•œ ë°©ë²•ì€ ì›ë³¸ ì´ë¯¸ì§€ì˜ í‘œí˜„ ëŠ¥ë ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬í•œë‹¤.

ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Diffusion State Space Model(DiffuSSM)ì„ ê³ ì•ˆí•œë‹¤. DiffuSSMì—ì„œëŠ” Attention mechanismì„ ëŒ€ì‹ í•´ State Space Model(ìƒíƒœ ê³µê°„ ëª¨ë¸)ì„ ì´ìš©í•˜ì—¬ ì¢€ ë” í™•ì¥ ê°€ëŠ¥í•œ(scalable) Diffusion architectureë¥¼ êµ¬ì„±í•˜ì˜€ë‹¤.

DiffuSSM architectureëŠ” Diffusion processë¥¼ ì§„í–‰í•˜ë©´ì„œ, DDPMì˜ ë©”ì†Œë“œì¸ ì´ë¯¸ì§€ ì „ì—­ ì••ì¶•(global compression)ì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©´ì„œ ì´ë¯¸ì§€ì˜ ìì„¸í•œ í‘œí˜„ì„ ìœ ì§€í•œë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¶€ë™ ì†Œìˆ˜ì ì˜ ê³„ì‚°ì„ ìµœì í™”í•˜ëŠ” FLOP-eifficient architectureë¥¼ Diffusionì— ì ìš©í•˜ì—¬ ì—°êµ¬ì˜ ì¤‘ìš”í•œ ë‹¤ìŒ ìŠ¤í…ì„ ë°ŸëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì˜€ë‹¤.

DiffuSSMì€ ImageNetê³¼ LSUN datasetì—ì„œì˜ ê´‘ë²”ìœ„í•œ í‰ê°€ë¥¼ í†µí•´ ê¸°ì¡´ì˜ Attention moduleì„ ì´ìš©í•˜ëŠ” diffusion modelê³¼ ë¹„êµí•˜ì—¬ FID ë° Inception Score metricì—ì„œ ë™ë“±í•˜ê±°ë‚˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, FLOPS ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì„ ë³´ì˜€ë‹¤.

# 1. Introduction

Denoising Diffusion Probabilistic Model(DDPM)ì˜ ë“±ì¥ìœ¼ë¡œ ì¸í•´ ì´ë¯¸ì§€ ìƒì„± ë¶„ì•¼ì—ì„œ ë§ì€ ë°œì „ì´ ì¼ì–´ë‚¬ë‹¤.

DDPMì€ latent variable(ì ì¬ ë³€ìˆ˜)ì˜ denoisingí•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µì ìœ¼ë¡œ ì§„í–‰í•œë‹¤. ë°˜ë³µì ì¸ denoisingì„ ì¶©ë¶„í•˜ê²Œ ì§„í–‰í•´ì¤Œìœ¼ë¡œì¨ ê³ í™”ì§ˆì˜ ìƒ˜í”Œì„ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ° ê¸°ëŠ¥ì„ í†µí•´ ë³µì¡í•œ ì‹œê°ì  ë¶„í¬ë¥¼ í¬ì°©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

ê·¸ëŸ¬ë‚˜ DDPMì„ ë” ë†’ì€ í•´ìƒë„ë¡œ í™•ì¥í•˜ëŠ” ë°ì—ëŠ” ë§¤ìš° í° Computational challengeê°€ ì¡´ì¬í•œë‹¤. 

ê°€ì¥ í° ë³‘ëª©(bottleneck)ìœ¼ë¡œëŠ” ê³ í™”ì§ˆì˜ ìƒì„±ì„ ì§„í–‰í•  ë•Œ **<U>Self-attention</U>**ì— ì˜ì¡´í•œë‹¤ëŠ” ì ì´ ìˆë‹¤.

U-Net architectureì˜ ê²½ìš°, ì´ ë³‘ëª© í˜„ìƒì€ **<U>ResNetê³¼ Attention layerë¥¼ ê²°í•©í•˜ëŠ” ë°ì—ì„œ ë°œìƒ</U>**í•œë‹¤. 

DDPMì˜ ì„±ëŠ¥ì€ GAN(Generative Adversarial Networks)ì„ ëŠ¥ê°€í•˜ì§€ë§Œ, DDPM architectureëŠ” Multi-head attention layerë¥¼ í•„ìš”ë¡œ í•œë‹¤.

Transformer modelì˜ êµ¬ì¡°ì—ì„œ Attention mechanismì€ ëª¨ë¸ì˜ ì¤‘ì‹¬ êµ¬ì„± ìš”ì†Œì„ì„ ëˆ„êµ¬ë„ ë¶€ì •í•  ìˆ˜ ì—†ì„ ê²ƒì´ê³ , ì´ë¯¸ì§€ í•©ì„±ì—ì„œ ì´ë¯¸ì§€ í•©ì„±ì˜ SOTAë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤.

<span style="color:#BA6835">Attentionì˜ ê³„ì‚° ë³µì¡ë„ëŠ” **<U>input sequence (length of $n$)ì— ëŒ€í•´ì„œ 2ì°¨ì (quadratic, $O(n^2)$)ìœ¼ë¡œ ì¦ê°€</U>**</span> í•˜ë©°, ì´ëŠ” ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ë£° ë•Œ **<U>ë„ˆë¬´ ë§ì€ ê³„ì‚°ì„ ìš”êµ¬</U>**í•˜ê²Œ ëœë‹¤.

ì´ëŸ° Computational costë¡œ ì¸í•´, ì°¨ì›ì„ ì••ì¶•í•˜ì—¬ ì´ë¯¸ì§€ í‘œí˜„ì„ ì••ì¶•í•˜ëŠ” Representation compression methodê°€ ë“±ì¥í•˜ê²Œ ë˜ì—ˆë‹¤.

ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” architectureëŠ” ì¼ë°˜ì ìœ¼ë¡œ <U>Patchifying</U> ë˜ëŠ” <U>Multi-scale resolution</U>ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ í•˜ë„ë¡ í•˜ì˜€ë‹¤.

Patchifyingì€ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê¸´ í•˜ì§€ë§Œ, **<U>ê³ ì£¼íŒŒ ì˜ì—­ì˜ ì¤‘ìš”í•œ ê³µê°„ì  ì •ë³´</U>**(critical high-frequency spatial information)ì™€ **<U>êµ¬ì¡°ì  ë¬´ê²°ì„±</U>**(structural integrity)ì„ ì €í•˜ì‹œí‚¨ë‹¤. 

Multi-scale resolutionì€ Attention layerì—ì„œì˜ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê¸´ í•˜ì§€ë§Œ, downsamplingì„ í†µí•´ ê³µê°„ì  ë””í…Œì¼ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆê³  upsamplingì„ ì ìš©í•  ë•Œ artifactë¥¼ ë„ì…í•  ìˆ˜ ìˆë‹¤.

- **Architecture of DiffuSSM**

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled.png)

Attention-free architectureì¸ DiffuSSMì€ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ ì‘ì—…ì—ì„œ Attention ëŒ€ì‹  ë‹¤ë¥¸ architectureë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ Computational costë¥¼ ì¤„ì¸ë‹¤.

DiffuSSMì€ Diffusion processì—ì„œ Gated Space State Model(using S4D)ì„ backboneìœ¼ë¡œ ì´ìš©í•œë‹¤.

ì´ì „ ì‘ì—…ì—ì„œ SSM ê¸°ë°˜ì˜ sequence modelì´ general-purposeì˜ Neural sequence modelë³´ë‹¤ ë›°ì–´ë‚œ ê²ƒì„ ì¦ëª…í•´ ì™”ë‹¤.

ì´í›„ ì„œìˆ ë˜ëŠ” **<B>4.DiffuSSM</B>**ì—ì„œ DiffuSSMì˜ architectureì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.

# 2. Related Work

### Diffusion Models

DDPMì€ diffusionì— ê¸°ë°˜í•œ ëª¨ë¸ë“¤ì— ì—„ì²­ë‚œ ë°œì „ì„ ê°€ì ¸ì™”ë‹¤. ì´ë¯¸ì§€ ìƒì„± taskì—ì„œ diffusion-based model ì´ì „ì—ëŠ” GANì— ê¸°ë°˜í•œ ëª¨ë¸ë“¤ì´ ì„ í˜¸ë˜ì—ˆë‹¤. diffusionê³¼ score-based modelë“¤ì€ ì´ë¯¸ì§€ ìƒì„± taskì—ì„œ ì—„ì²­ë‚œ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤. DDPMì—ì„œì˜ ëˆˆë¶€ì‹  ë°œì „ì˜ í‚¤ í¬ì¸íŠ¸ì—ëŠ” sampling methodë¥¼ ë°œì „ì‹œí‚¨ ê²ƒê³¼, Classification-free guidance ë‘ ê°€ì§€ê°€ ìˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ, [Song et al.ì€ DDIM(Denoising Diffusioin Implicit Model)ì„ ì œì‹œí•¨](https://arxiv.org/abs/2010.02502)ìœ¼ë¡œì¨ diffusion modelì˜ sampling ê³¼ì •ì„ ë” ë¹ ë¥´ê²Œ ì§„í–‰í•˜ë„ë¡ ë°œì „ì‹œì¼°ë‹¤.

ì ì¬ ê³µê°„(Latent Space)ì˜ ëª¨ë¸ë§ì€ deep generation modelì—ì„œì˜ ë˜ ë‹¤ë¥¸ í•µì‹¬ í…Œí¬ë‹‰ ì¤‘ í•˜ë‚˜ì´ë‹¤. VAE(Variational Auto Encoder)ëŠ” encoder-decoder êµ¬ì¡°ë¥¼ í†µí•´ ì ì¬ ê³µê°„ì„ í•™ìŠµí•˜ëŠ” ì„ êµ¬ì ì¸ ëª¨ë¸ì´ë‹¤. ì´ ëª¨ë¸ì—ì„œì˜ compression ideaê°€ ìµœê·¼ì— Stable Diffusionìœ¼ë¡œ ë¶ˆë¦¬ëŠ” LDM(Latent Diffusion Models)ì—ì„œë„ ì ìš©ë˜ê³  ìˆë‹¤.

### Architectures for Diffusion Models

ì´ˆê¸°ì˜ Diffusion modelì€ U-Net ìŠ¤íƒ€ì¼ì˜ architectureë¥¼ í™œìš©í•˜ì˜€ë‹¤. ì´ì–´ì§€ëŠ” í›„ì† ì—°êµ¬ë“¤ì€ Multi-Scale í•´ìƒë„ ìˆ˜ì¤€ì—ì„œ ë” ë§ì€ Attention layerë¥¼ ì¶”ê°€, Residual connection, Normalization ë“±ì˜ ê¸°ìˆ ì„ ì´ìš©í•˜ì—¬ U-Netì„ ê°œì„ í•˜ì˜€ë‹¤.

ê·¸ëŸ¬ë‚˜ U-Netì€ Attention mechanismì—ì„œ ê³„ì† ì¦ê°€í•˜ëŠ” Computational costë•Œë¬¸ì— ë†’ì€ í•´ìƒë„ë¡œ í™•ì¥í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆë‹¤. 

ìµœê·¼ ViT(Vision Transformer)ë¼ëŠ” ê°•ë ¥í•œ í™•ì¥ ì†ì„±ê³¼ ì¥ê¸° ë²”ìœ„ ëª¨ë¸ë§ ëŠ¥ë ¥ì„ ê³ ë ¤í•˜ì—¬ ëŒ€ì²´ Architectureë¡œ ë“±ì¥í•˜ì˜€ë‹¤. ì´ëŠ” Convolution inductive biasê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

Diffusion TransformersëŠ” êµ‰ì¥íˆ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. ë˜ ë‹¤ë¥¸ hybrid CNN-Transformer Architecture ë˜í•œ í•™ìŠµ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì œì•ˆë˜ì—ˆë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ìˆœì°¨ ëª¨ë¸ì˜ íƒìƒ‰ê³¼ ê´€ë ¨ëœ ë””ìì¸ì„ í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì— ì¤‘ì ì„ ë‘ë©°, ì™„ì „í•œ Attention-free architectureë¥¼ ëª©í‘œë¡œ í•œë‹¤.

### Efficient Long Range Sequence Architectures

ê¸°ë³¸ì ì¸ Transformer architectureëŠ” <U>inputìœ¼ë¡œ ì£¼ì–´ì§„ sequenceë¥¼ tokenizeí•œ í›„ <span style="color:#BA6835">individual tokenë“¤ì˜ ë¬¸ë§¥ì  ê´€ê³„</span>ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ Attention mechanismì„ ì´ìš©</U>í•œë‹¤. 

ê·¸ëŸ¬ë‚˜ $N$ê°œì˜ tokenì´ ì£¼ì–´ì¡Œì„ ë•Œ Attention mechanismì˜ Computational costëŠ” quadratic($O(N^2)$)ì´ë¯€ë¡œ, **ë§¤ìš° ê¸´ sequenceë¥¼ ë§Œë‚  ê²½ìš° í°bottleneckì„ ì ‘í•˜ê²Œ ëœë‹¤.**

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Self-attentionì„ $O(N^2)$ ì´í•˜ì˜ Computational costë¡œ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ Attention-approximation methodê°€ ë„ì…ë˜ì—ˆë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ Megaì—ì„  ì§€ìˆ˜ ì´ë™ í‰ê· (Exponential Moving Average)ê³¼ ë‹¨ìˆœí™”ëœ Attention unitì„ ê²°í•©í•˜ì—¬ Transformerì˜ ê¸°ë³¸ ì„±ëŠ¥ì„ ëŠ¥ê°€í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤.

ì—°êµ¬ìë“¤ì€ ì „í†µì ì¸ Transformer modelì„ ë„˜ì–´ ê¸´ ì‹œí€€ìŠ¤(elongated sequences)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ëŠ¥ìˆ™í•œ ëŒ€ì²´ modelë“¤ë„ íƒêµ¬í•˜ì˜€ë‹¤. ê·¸ ì¤‘ì—ì„œ State Space Modelì— ê¸°ë°˜í•œ(SSM-based) model architectureëŠ” LRA ë° Audio benchmarkì—ì„œ í˜„ëŒ€ì˜ SOTA methodë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.

# 3. Preliminaries

## 3.1. Diffusion Models

### DDPM (Denoising Diffusion Probabilistic Models) Architecture

DDPM(Denoising Diffusion Probabilictic Models)ì€ ì£¼ì–´ì§„ ë°ì´í„° ë¶„í¬ë¥¼ í•™ìŠµí•˜ê³  ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ìƒì„±ëª¨ë¸ì´ë‹¤. DDPMì€ Noise ì¶”ê°€ì™€ ì œê±° ê³¼ì •ì„ í†µí•´ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. 

DDPMì´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì£¼ìš” ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

### 1. Forward Process

<U>Forward Process</U>ì—ì„œëŠ” <span style="color:#BA6835">ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ Gaussian Noiseë¥¼ ì¶”ê°€í•œë‹¤.</span> ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

$$q(\mathbf{x}_t \| \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) $$

ì—¬ê¸°ì„œ:

- $\mathbf{x}_t$ : ì‹œê°„  $t$ì—ì„œì˜ ë°ì´í„° ìƒíƒœ
- $\beta_t$ : ì‹œê°„  $t$ì—ì„œì˜ Noise Scale
- $\mathcal{N}$ : Gaussian Distribution (=ì •ê·œë¶„í¬)

### 2. Reverse Process

<U>Reverse Process</U>ì—ì„œëŠ” <span style="color:#BA6835">Noiseë¥¼ ì œê±°í•˜ì—¬ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µì›</span>í•œë‹¤. ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤.

$$p_\theta(\mathbf{x}_{t-1} \| \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu\theta(\mathbf{x}_t, t), \Sigma\theta(\mathbf{x}_t, t))$$

- $\mu_\theta$,  $\Sigma_\theta$ : parameterizeëœ í‰ê· ê³¼ ë¶„ì‚°

### 3. Loss Function

ëª¨ë¸ í•™ìŠµì„ ìœ„í•´, ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$L = \mathbb{E}{q} \left[ \sum{t=1}^T \text{KL}(q(\mathbf{x}_{t-1} \| \mathbf{x}_t, \mathbf{x}_0) \parallel p\theta(\mathbf{x}_{t-1} \| \mathbf{x}_t)) \right]$$

- $\text{KL}$ : Kullback-Leibler Divergence
- $q$ : Forward Process ì˜ ë¶„í¬
- $p_\theta$ : Reverse Process ì˜ ë¶„í¬

### ìš”ì•½

DDPMì€ ë°ì´í„°ì— ì ì§„ì ìœ¼ë¡œ Noiseë¥¼ ì¶”ê°€í•˜ê³  ì´ë¥¼ ì œê±°í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. ì´ ê³¼ì •ì€ <U>ìˆœë°©í–¥ ê³¼ì •(forward process)</U>ê³¼ <U>ì—­ë°©í–¥ ê³¼ì •(reverse process)</U>ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ëª¨ë¸ì€ KL-Divergenceë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµëœë‹¤. Noise ì¶”ê°€ì™€ ì œê±° ê³¼ì •ì€ Gaussian Distributionë¡œ ëª¨ë¸ë§ë˜ë©°, ê° ë‹¨ê³„ëŠ” ì‹œê°„ì— ë”°ë¼ ë‹¤ë¥¸ Noise scheduleì„ ê°€ì§„ë‹¤.

## 3.2 Architecture for Diffusion Models

Diffusion Modelsë¥¼ ìœ„í•œ ì•„í‚¤í…ì²˜ ë¶€ë¶„ì—ì„œëŠ” ë°ì´í„°ì˜ ë†’ì´ $H$, ë„ˆë¹„ $W$, ê·¸ë¦¬ê³  í¬ê¸° $C$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $\mu_{\theta}$ë¥¼ parameterizingí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤. ì´ parameterizingì€  $\mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{H \times W \times C}$ ì˜ mappingì„ ìˆ˜í–‰í•œë‹¤.

ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì—ì„œ ì›ì‹œ í”½ì…€(raw pixels) ë˜ëŠ” ì‚¬ì „ í•™ìŠµëœ VAE Encoderì—ì„œ ì¶”ì¶œëœ Latent Space Representationì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. <U>ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•ŒëŠ” Latent Spaceì—ì„œë„ $H$ì™€ $W$ê°€ í¬ê¸° ë•Œë¬¸ì—,</U> ì´ í•¨ìˆ˜ê°€ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ íŠ¹ìˆ˜í•œ architectureê°€ í•„ìš”í•˜ë‹¤.

### U-Nets with Self-attention

U-Net architectureëŠ” ì—¬ëŸ¬ í•´ìƒë„ì—ì„œ í•©ì„±ê³±ê³¼ í•˜ìœ„ ìƒ˜í”Œë§(sub-sampling)ì„ ì‚¬ìš©í•˜ì—¬ ê³ í•´ìƒë„ ì…ë ¥ì„ ì²˜ë¦¬í•œë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, self-attention ë ˆì´ì–´ëŠ” ì €í•´ìƒë„ ë¸”ë¡ì—ì„œ ì‚¬ìš©ëœë‹¤. í˜„ì¬ê¹Œì§€ self-attentionì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìµœì‹  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” U-Net ê¸°ë°˜ diffusion ëª¨ë¸ì€ ì—†ë‹¤. 

ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

1. $t_1, \ldots, t_T$ëŠ” imageì˜ sub-samplingìœ¼ë¡œ ìƒì„±ëœ ë‚®ì€ í•´ìƒë„ì˜ Feature map seriesë‹¤.
2. ê° ìŠ¤ì¼€ì¼ì—ì„œ ResNetì´ ì ìš©ëœë‹¤:
 $\mathbb{R}^{H_t \times W_t \times C_t}$ 
3. ì´ëŸ¬í•œ Feature mapì€ Upsamplingë˜ì–´ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ concatëœë‹¤.
4. ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ Self-attention layerëŠ” flattenëœ ìµœì € í•´ìƒë„ì—ì„œë§Œ ì ìš©ëœë‹¤.
5. Feature mapì€ $H_t W_t$ vectorì˜ sequenceë¡œ flattenëœë‹¤.

### Transformers with Patchification

Global contextualizationì—ì„œ self-attentionì„ ì´ìš©í•˜ëŠ” ê²ƒì€ diffusion ëª¨ë¸ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•˜ë‹¤. ë”°ë¼ì„œ, ì „ì²´ì ìœ¼ë¡œ self-attentionì— ê¸°ë°˜í•œ ì•„í‚¤í…ì²˜ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ë‹¤.

Transformer ì•„í‚¤í…ì²˜ëŠ” self-attentionì„ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì²˜ë¦¬í•œë‹¤. ì—¬ê¸°ì„œ Trasformerë¥¼ ì´ìš©í•˜ì—¬ ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ patchification ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤:

1. íŒ¨ì¹˜ í¬ê¸° $P$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, TransformerëŠ” ì´ë¯¸ì§€ë¥¼ $P \times P$ ì°¨ì›ì˜ patchë¡œ ë‚˜ëˆˆë‹¤:
 $\mathbb{R}^{H/P \times W/P \times C'}$ 
2. íŒ¨ì¹˜ í¬ê¸° $P$ëŠ” ì´ë¯¸ì§€ì™€ ê³„ì‚° ìˆ˜ìš”ì˜ ìœ íš¨í•œ granularity(ì„¸ë¶„ì„±)ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
3. ì´ë¯¸ì§€ë¥¼ Transformerì— ì…ë ¥í•˜ê¸° ìœ„í•´, ì´ë¯¸ì§€ë¥¼ Flattení•˜ê³  ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ $(HW)/P^2$ hidden vectorë¥¼ ì–»ëŠ”ë‹¤ :
 $\mathbb{R}^{(HW)/P^2}$ 
4. í° íŒ¨ì¹˜ í¬ê¸°ëŠ” ê³µê°„ì  ì„¸ë¶€ ì‚¬í•­ì˜ ì†ì‹¤ì„ ì´ˆë˜í•  ìˆ˜ ìˆì§€ë§Œ, ê³„ì‚° íš¨ìœ¨ì„±ì„ ì œê³µí•œë‹¤.

# 4. DiffuSSM

## 4.1. State Space Models (SSMs)

### Definition of State Space Model

- ê¸°ì´ˆ ê°œë…
    - ë¬¼ë¦¬í•™ì  ê³„(system)ë¥¼ <U>ì…ë ¥(input)</U>, <U>ì¶œë ¥(output)</U>, <U>ìƒíƒœ ë³€ìˆ˜(state variable)</U>ì˜ <span style="color:#BA6835">1ì°¨ ìƒë¯¸ë¶„ ë°©ì •ì‹(1st-order ODE)</span>ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ìˆ˜í•™ì  ëª¨ë¸. ê¸°ì¡´ì—ëŠ” ì „ê¸°ì „ì ê³µí•™ ë¶„ì•¼ì˜ ì œì–´ ì´ë¡ ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.
    - $h'(t) = Ah(t) + Bx(t)$          $h$ : hidden state, $x$ : input sentence
        
      $y(t) = Ch(t) + Dx(t)$           $y$ : output sentence,  $A,B, C, D$ : (Learnable \| Fixed) Parameter
        
    
    <aside>
    ğŸ’¡ ê¸°ì¡´ ì œì–´ ì´ë¡ ì—ì„œëŠ” $A, B, C, D$ë¥¼ ê³ ì •. í•˜ì§€ë§Œ Machine Learningì—ì„  <U>$A, B, C, D$ë¥¼ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì!</U>
    
    </aside>
    

### Continuous signal to Discrete signal

ê¸°ì¡´ì˜ SSMì€ input signalì„ continuous sequenceë¡œ ì…ë ¥ë°›ê²Œ ì„¤ê³„ë˜ì—ˆì§€ë§Œ, ìš°ë¦¬ê°€ ì»´í“¨í„°ë¡œ signalì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ <span style="color:#BA6835">**<U>continuous sequence</U>**ë¥¼ **<U>discrete sequence</U>**ë¡œ ë³€í™˜í•´ ì£¼ì–´ì•¼ í•œë‹¤.</span>

Discretizationì˜ ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€(Zero-order hold, Bilinear transformation, Backward Euler method, etcâ€¦)ê°€ ìˆì§€ë§Œ ê·¸ ì¤‘ì—ì„œ **<U>Zero-order hold</U>**ì™€ **<U>Bilinear transformation </U>**ë‘ ê°€ì§€ë¥¼ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.

- **Zero-order hold, ZOH (ì˜ì°¨ ìœ ì§€)** â†’ **<U>mamba</U>**ì—ì„œ ì‚¬ìš©

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%201.png)

<U>ì´ì‚° ì‹ í˜¸ë¥¼ ë°›ì„ ë•Œë§ˆë‹¤, ìƒˆë¡œìš´ ì´ì‚° ì‹ í˜¸ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ê·¸ ê°’ì„ ìœ ì§€í•œë‹¤.</U>

â†’ <span style="color:#BA6835">ì…ë ¥ ì‹ í˜¸ $u(t)$ê°€ sampling ê°„ê²© ë™ì•ˆ ì¼ì •í•˜ë‹¤!</span>

- SSMì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ê°€ **<U>ì¼ì°¨ ìƒë¯¸ë¶„ ë°©ì •ì‹</U>**ì„ì„ ì´ìš©í•˜ë©´,
    - ì´ì‚° ì‹œê°„ SSMì—ì„œ sampling ì£¼ê¸° $T$ í›„ì˜ State vector $x[k+1]$ë¥¼ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
        
        (Zero-order holdì— ë”°ë¼ <U>ì…ë ¥ $u(t)$ê°€ ì£¼ê¸° ë™ì•ˆ ì¼ì •í•˜ë‹¤</U>ê³  ê°€ì •)
        
        $x((k+1)T) = e^{A T} x(kT) + \int_{0}^{T} e^{A(T-\tau)} B u(kT) \, d\tau$
        
        $t = kT, t+T = (k+1)T$
        
        $A_d = e^{A T}$
        
        $B_d = \left( \int_{0}^{T} e^{A(T-\tau)} \, d\tau \right) B = A^{-1} (e^{A T} - I) B$
        
    
    ë¡œ parameterë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
    

ë‹¨, $A^{-1}$ì´ í•„ìš”í•˜ë‹¤! â†’ **<U>A matrixê°€ Invertibleí•´ì•¼ í•œë‹¤!</U>**

- **Bilinear transform (==Tustinâ€™s method, ìŒì„ í˜• ë³€í™˜) â†’ S4ì—ì„œ ì‚¬ìš©**

Input signalì˜ Continuous ì‹œê°„ ì‹œìŠ¤í…œì˜ Laplace domain(S-domain)ì„ Discrete ì‹œê°„ ì‹œìŠ¤í…œì˜ Z-domainìœ¼ë¡œ ë³€í™˜í•œë‹¤.

ë³€í™˜ ê³µì‹

 $s = \frac{2}{T} \frac{1 - z^{-1}}{1 + z^{-1}}$

- $s$ : ì—°ì† ì‹œê°„ Laplace domain function
- $z$ : ì´ì‚° ì‹œê°„ Z-domain function
- $T$ : sampling ì£¼ê¸°

### Recursive view of an SSM - using Bilinear Transform

- **Trapezoidal method**
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%202.png)
    
    Continuous signalì„ discretizeí•˜ê¸° ìœ„í•´, <span style="color:#BA6835">ì—°ì†í•¨ìˆ˜ì˜ ì ë¶„ ê°’ì„ ê·¼ì‚¬í•˜ëŠ” ìˆ˜ì¹˜ ì ë¶„(Numerical Integration)</span>ì„ ì´ìš©í•˜ì—¬ì•¼ í•œë‹¤.
    
    í•¨ìˆ˜ $f$ê°€ êµ¬ê°„ $[t_n, t_{n+1}]$ì— ì •ì˜ëœ ëŒ€í‘œ ê³¡ì„  ì•„ë˜ì˜ ì˜ì—­ì„ ì‚¬ë‹¤ë¦¬ê¼´ë¡œ ë™í™”ì‹œí‚¤ê³  ê·¸ ë©´ì ì„ ê³„ì‚°í•˜ëŠ” ì›ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” Trapezoidal method (ì‚¬ë‹¤ë¦¬ê¼´ ë°©ë²•)ì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤.
    
    $\int_{a}^{b} f(x) \, dx \approx \frac{b - a}{2} \left[ f(a) + f(b) \right]$
    
    êµ¬ê°„ì„ $n$ê°œì˜ ì‘ì€ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ë©´:
    
    $\int_{a}^{b} f(x) \, dx \approx \frac{h}{2} \left[ f(x_0) + 2 \sum_{i=1}^{n-1} f(x_i) + f(x_n) \right]$
    
    $\therefore$   $T : T = (t_{n+1} - t_n) \frac{f(t_n) + f(t_{n+1})}{2}$ 
    
- **Discretization (ì´ì‚°í™” ê³¼ì •) - Bilinear transform**
    
    Trapezoidal methodë¥¼ ì´ìš©í•´ SSMì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
    $x_{n+1} - x_n = \frac{1}{2} \Delta (f(t_n) + f(t_{n+1}))$
    
    ì—¬ê¸°ì„œ $\Delta = t_{n+1} - t_n$ ì´ë‹¤.
    
    ë§Œì•½  $x'_n = A x_n + B u_n$ (SSM ìˆ˜ì‹ì˜ ì²« ë²ˆì§¸ ì¤„)ì´ $f$ì— í•´ë‹¹í•œë‹¤ê³  í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ì„ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.
    
    $x_{n+1} = x_n + \frac{\Delta}{2} (A x_n + B u_n + A x_{n+1} + B u_{n+1})$
    
    ì´ë¥¼ ë³€í˜•í•˜ë©´
    
    $x_{n+1} - \frac{\Delta}{2} A x_{n+1} = x_n + \frac{\Delta}{2} A x_n + \frac{\Delta}{2} B (u_{n+1} + u_n)$
    
    ì´ê³ ,
    
    í•œ ë²ˆ ë” ìœ„ì˜ ì‹ì„ ì •ë¦¬í•˜ë©´
    
    $(I - \frac{\Delta}{2} A) x_{n+1} = (I + \frac{\Delta}{2} A) x_n + \frac{\Delta}{2} B (u_{n+1} + u_n)$ ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
    
    ë”°ë¼ì„œ, ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.
    
    $x_{n+1} = (I - \frac{\Delta}{2} A)^{-1} (I + \frac{\Delta}{2} A) x_n + (I - \frac{\Delta}{2} A)^{-1} \frac{\Delta}{2} B (u_{n+1} + u_n)$
    
    ì—¬ê¸°ì„œ, <U>ZOHë¥¼ ì ìš©í•˜ë©´  $u_{n+1} \approx u_n$</U> (ì œì–´ ë²¡í„°ëŠ” ì‘ì€ $\Delta$ ë™ì•ˆ ì¼ì •í•˜ë‹¤ê³  ê°€ì •).
    

ì´ë ‡ê²Œ Bilinear transformì„ ì´ìš©í•œ Discretized SSMì˜ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ì˜€ë‹¤. 

ì´ ëª¨ë¸ì„ ì™„ì „íˆ ëª…í™•í•˜ê²Œ í•˜ê¸° ìœ„í•´, ìˆ˜ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

1. **ì´ì‚°í™”ëœ ì‹œìŠ¤í…œ ë°©ì •ì‹**:
 $x_{n+1} = x_n + \frac{\Delta}{2} (A x_n + B u_n + A x_{n+1} + B u_{n+1})$

    
    
    ì´ë¥¼ ì¬ì •ë ¬í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
    $x_{n+1} - \frac{\Delta}{2} A x_{n+1} = x_n + \frac{\Delta}{2} A x_n + \frac{\Delta}{2} B (u_{n+1} + u_n)$
    
    ê·¸ë¦¬ê³  Discretized SSMì˜ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆë‹¤. 
    $$
    \begin{aligned}
    x_{n+1} - x_n &= \frac{\Delta}{2} (A x_n + A x_{n+1} + B (u_{n+1} + u_n)) \\
    (I - \frac{\Delta}{2} A) x_{n+1} &= (I + \frac{\Delta}{2} A) x_n + \Delta B u_{n+1} \\
    x_{n+1} &= (I - \frac{\Delta}{2} A)^{-1} (I + \frac{\Delta}{2} A) x_n + (I - \frac{\Delta}{2} A)^{-1} \Delta B u_{n+1}
    \end{aligned}
    $$

    ì—¬ê¸°ì„œ $u_{n+1} \approx u_n$ì´ë¼ê³  ê°€ì •í•œë‹¤ (ì œì–´ ë²¡í„°ëŠ” ì‘ì€ $\Delta$ì— ëŒ€í•´ ìƒìˆ˜ë¡œ ê°€ì •).
    
2. **Discretized parameter**:
    
    
    $\bar{A} = (I - \frac{\Delta}{2} A)^{-1} (I + \frac{\Delta}{2} A)$
    $\bar{B} = (I - \frac{\Delta}{2} A)^{-1} \Delta B$
    $\bar{C} = C$
    
3. **ì´ì‚°í™”ëœ ëª¨ë¸ì˜ ìµœì¢… í˜•íƒœ**:
    
    $x_k = \bar{A} x_{k-1} + \bar{B} u_k$
    $y_k = \bar{C} x_k$
    

### Recurrent  Visualization

ê° Timestepì—ì„œ, SSMì€ <U>í˜„ì¬ ì…ë ¥ì´ ì´ì „ ìƒíƒœì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ê³„ì‚°</U>í•œ ë‹¤ìŒ ì˜ˆì¸¡ëœ ì¶œë ¥ì„ ê³„ì‚°í•œë‹¤.

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%203.png)

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%204.png)

ì´ ê³„ì‚° ë©”ì»¤ë‹ˆì¦˜ì€ **<U><span style="color:#BA6835">RNN</span></U>**ì˜ ë°©ì‹ê³¼ ë˜‘ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.

### **Convolutive view of an SSM**

ì•ì„œ ì„¤ëª…í•œ SSMì˜ recurrenceëŠ” í•©ì„±ê³±ìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ SSMì˜ ë°©ì •ì‹ì„ ë°˜ë³µí•œë‹¤.

 $x_k = \bar{A} x_{k-1} + \bar{B} u_k$ 
 $y_k = \bar{C} x_k$ 

- ì‹œìŠ¤í…œì˜ ì²« ë²ˆì§¸ ì¤„ë¶€í„° ì‹œì‘í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
    
    **0ë‹¨ê³„** :  $x_0 = \bar{B} u_0$
    
    **1ë‹¨ê³„** : $x_1 = \bar{A} x_0 + \bar{B} u_1 = \bar{A} \bar{B} u_0 + \bar{B} u_1$ 
    
    **2ë‹¨ê³„** : $x_2 = \bar{A} x_1 + \bar{B} u_2 = \bar{A} (\bar{A} \bar{B} u_0 + \bar{B} u_1) + \bar{B} u_2 = \bar{A}^2 \bar{B} u_0 + \bar{A} \bar{B} u_1 + \bar{B} u_2$ 
    
    $x_k$ë¥¼ $(u_0, u_1, ..., u_k)$ë¡œ parameterized function $f$ë¡œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.
    
- ì´ì œ ì‹œìŠ¤í…œì˜ ë‘ ë²ˆì§¸ ì¤„ë¡œ ë„˜ì–´ê°€ì„œ, ë°©ê¸ˆ ê³„ì‚°í•œ $x_k$ê°’ì„ ì£¼ì…í•  ìˆ˜ ìˆë‹¤.
    
    **0ë‹¨ê³„** : $y_0 = \bar{C} x_0 = \bar{C} \bar{B} u_0$
    
    **1ë‹¨ê³„** :  $y_1 = \bar{C} x_1 = \bar{C} (\bar{A} \bar{B} u_0 + \bar{B} u_1) = \bar{C} \bar{A} \bar{B} u_0 + \bar{C} \bar{B} u_1$ 
    
    **2ë‹¨ê³„**:   $y_2 = \bar{C} x_2 = \bar{C} (\bar{A}^2 \bar{B} u_0 + \bar{A} \bar{B} u_1 + \bar{B} u_2) = \bar{C} \bar{A}^2 \bar{B} u_0 + \bar{C} \bar{A} \bar{B} u_1 + \bar{C} \bar{B} u_2$
    

> ê° ë‹¨ê³„ì˜ ê³„ì‚°ì—ì„œ ì¼ì •í•œ patternì´ ë³´ì´ë¯€ë¡œ, ì´ ê·œì¹™ì„ ì´ìš©í•´ ë‹¨ê³„ì ì¸ ì „ì²´ì˜ ê³„ì‚°ì„ í•œ ë²ˆì— í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?
> 

ì—¬ê¸°ì„œ, í•©ì„±ê³± ì»¤ë„  $\bar{K}_k = (\bar{C} \bar{B}, \bar{C} \bar{A} \bar{B}, ..., \bar{C} \bar{A}^{k} \bar{B})$ ì„  $u_k$ ì— ì ìš©í•˜ì—¬ $K * u$ ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%205.png)

í–‰ë ¬ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, í•©ì„±ê³± ì»¤ë„ì´ ì´ì‚°í™” ê³¼ì • í›„ ì–»ì–´ì§„ ê²ƒì„ì„ ëª…ì‹œí•˜ê¸° ìœ„í•´  $\bar{K}$ ì— barë¥¼ í‘œê¸°í•˜ì—¬ ì ìš©í•œë‹¤.

ì´ëŠ” ë…¼ë¬¸ì—ì„œ SSM í•©ì„±ê³± ì»¤ë„(convolutive kernel)ì´ë¼ê³  í•˜ë©°, ê·¸ í¬ê¸°ëŠ” ì „ì²´ input sequenceì™€ ë™ì¼í•˜ë‹¤.

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%206.png)

ì´ í•©ì„±ê³± ì»¤ë„ì€ <U>Fast Fourier Transform</U>ì„ í†µí•´ ê³„ì‚° ë³µì¡ë„ë¥¼ ìµœì í™”í•˜ì—¬ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤.

### Fourier Transformation, Fast Fourier Transform(FFT)

- **Fourier Series (í‘¸ë¦¬ì— ê¸‰ìˆ˜)**
    - **ì •ì˜**
    
    ì£¼ê¸°ì ì¸ í•¨ìˆ˜ ğ‘“(ğ‘¡)ë¥¼ ì£¼ê¸° ğ‘‡ë¡œ ë‚˜íƒ€ë‚¼ ë•Œ, í‘¸ë¦¬ì— ê¸‰ìˆ˜ëŠ” ì´ í•¨ìˆ˜ë¥¼ <span style="color:#BA6835">ì‚¼ê° í•¨ìˆ˜ì˜ í•©</span>ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤.
    
    - **ìˆ˜ì‹**
    
    ì£¼ê¸° í•¨ìˆ˜ ğ‘“(ğ‘¡)*f*(*t*)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.
    
    $f(t) = a_0 + \sum_{n=1}^{\infty} \left( a_n \cos\left(\frac{2\pi nt}{T}\right) + b_n \sin\left(\frac{2\pi nt}{T}\right) \right)$
    
    ì—¬ê¸°ì„œ ê³„ìˆ˜ $a_n$ì™€ $b_n$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
    
    $a_0 = \frac{1}{T} \int_{0}^{T} f(t) \, dt$
    $a_n = \frac{2}{T} \int_{0}^{T} f(t) \cos\left(\frac{2\pi nt}{T}\right) \, dt$
    
    $b_n = \frac{2}{T} \int_{0}^{T} f(t) \sin\left(\frac{2\pi nt}{T}\right) \, dt$
    
- **Fourier Transformation (í‘¸ë¦¬ì— ë³€í™˜)**
    - **ì •ì˜**
    
    ë¹„ì£¼ê¸°ì ì¸ í•¨ìˆ˜ ğ‘“(ğ‘¡)ë¥¼ ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, Fourier Transform (í‘¸ë¦¬ì— ë³€í™˜, FT)ì„ ì‚¬ìš©í•œë‹¤. ì´ ë³€í™˜ì€ ì‹œê°„ ë„ë©”ì¸ì—ì„œ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤.
    
    - **ìˆ˜ì‹**
    
    Fourier Transformì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
    
    $F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i\omega t}$$dt$
    
    ì´ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ input signalì¸ f(t)ê°€ ì—°ì†ì ì„(continuous)ì¸ ê²½ìš° ì •ì˜ëœ ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ì´ì‚°ì ì¸ ê²½ìš°ë¥¼ ê´€ì°°í•´ì•¼ í•˜ë¯€ë¡œ Discrete Fourier Transformì„ ë³´ì•„ì•¼ í•œë‹¤.
    
- **Discrete Fourier Transform**
    - ì •ì˜
    
    ì´ì‚° ì‹ í˜¸ë¥¼ ì£¼íŒŒìˆ˜ ì˜ì—­ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ DFT(Discrete Fourier Transform)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” í‘¸ë¦¬ì— ë³€í™˜ì˜ Discretized formì´ë‹¤.
    
    - ìˆ˜ì‹
    
    ê¸¸ì´ ğ‘ì¸ ì´ì‚° ì‹ í˜¸ ğ‘¥[ğ‘›]ì— ëŒ€í•´ DFTì˜ ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.
    
    $X[k] = \sum_{n=0}^{N-1} x[n] e^{-i\frac{2\pi}{N}kn}$
    
    Input sequenceë¥¼ $N$ê°œë¡œ ë‚˜ëˆ„ì–´ $N$ê°œì˜ tokenì— ëŒ€í•´ ê°ê° Në²ˆì˜ ê³±ì…ˆê³¼ ë§ì…ˆì„ ìˆ˜í–‰ â†’ <span style="color:#BA6835">$O(N^2)$ì˜ Time Complexity</span>
    
    **<U>ì—¬ê¸°ê¹Œì§€ ë³´ë©´ SSMê³¼ Attentionì˜ ê³„ì‚° ë³µì¡ë„ê°€ ê°™ì§€ë§Œ, ì´ ê³„ì‚° ë³µì¡ë„ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‹¤!</U>**
    

- **Cooley-Tukey FFT Algorithm**
    - ì •ì˜
    
    Cooley-Tukey FFTëŠ” **<U>ë¶„í•  ì •ë³µ(Divide-and-Conquer)</U>** ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ DFTë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤. Cooley-Tukey FFTì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ input signalì„ recursiveí•˜ê²Œ ì ˆë°˜ì”© ë‚˜ëˆ„ê³ , ê°ê°ì— ëŒ€í•´ FFTë¥¼ ê³„ì‚°í•œ í›„ ì´ë¥¼ ë”í•´ì¤€ë‹¤. 
    
    - ìˆ˜ì‹
    1. Length of $N$ì¸ discrete signal $x[n]$ì˜ DFTëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
        
        $X[k] = \sum_{n=0}^{N-1} x[n] e^{-i \frac{2\pi}{N} kn}$
        
    2. ì…ë ¥ ë°ì´í„°ì˜ ë¶„í• 
    $x_{\text{even}}[m] = x[2m] \quad \text{for} \quad m = 0, 1, \ldots, \frac{N}{2}-1$
    $x_{\text{odd}}[m] = x[2m+1] \quad \text{for} \quad m = 0, 1, \ldots, \frac{N}{2}-1$
    3. Even / Odd ë¶€ë¶„ì— ëŒ€í•œ DFTì˜ ê³„ì‚°
    $X_{\text{even}}[k] = \sum_{m=0}^{\frac{N}{2}-1} x_{\text{even}}[m] e^{-i \frac{2\pi}{\frac{N}{2}} km}$
    $X_{\text{odd}}[k] = \sum_{m=0}^{\frac{N}{2}-1} x_{\text{odd}}[m] e^{-i \frac{2\pi}{\frac{N}{2}} km}$
    4. ì „ì²´ DFT ê³„ì‚°
    $X[k] = X_{\text{even}}[k] + W_N^k X_{\text{odd}}[k]$
    
        
        $X[k + N/2] = X_{\text{even}}[k] - W_N^k X_{\text{odd}}[k]$
        
        - $W_N = e^{-i \frac{2\pi}{N}}$
- **Time Complexity of Cookey-Tukey FFT Algorithm**
    
    Cooley-Tukey FFTì˜ ì‹œê°„ ë³µì¡ë„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¬ê·€ ê´€ê³„ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
    
     $T(N) = 2T\left(\frac{N}{2}\right) + O(N)$ 
    
    ì—¬ê¸°ì„œ  $2T\left(\frac{N}{2}\right)$  ëŠ” ë‘ ê°œì˜ í•˜ìœ„ ë¬¸ì œ(ê¸¸ì´ê°€  $N/2$  ì¸ ë¶€ë¶„ ì‹ í˜¸)ì— ëŒ€í•´ FFTë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ê³ ,  $O(N)$  ì€ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ë° í•„ìš”í•œ ì‹œê°„ì´ë‹¤.
    

ë‹¤ìŒì€ Cooley-Tukeyì˜ FFT algorithmì„ ì¬ê·€ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ Time Complexityë¥¼ ì¦ëª…í•˜ëŠ” ê³¼ì •ì´ë‹¤.

$$
\begin{align*}
T(N) &= 2T\left(\frac{N}{2}\right) + O(N) \\
&=2 \left[ 2T\left(\frac{N}{4}\right) + O\left(\frac{N}{2}\right) \right] + O(N) \\
&=4T\left(\frac{N}{4}\right) + 2O\left(\frac{N}{2}\right) + O(N) \\
&=4 \left[ 2T\left(\frac{N}{8}\right) + O\left(\frac{N}{4}\right) \right] + 2O\left(\frac{N}{2}\right) + O(N) \\
&=8T\left(\frac{N}{8}\right) + 4O\left(\frac{N}{4}\right) + 2O\left(\frac{N}{2}\right) + O(N) \\
&\quad \vdots \\
& = NT(1) + O(N \log N)
\end{align*}
$$

ì—¬ê¸°ì„œ  $T(1) = O(1)$  ì´ë¯€ë¡œ, FFTì˜ ì „ì²´ ì‹œê°„ ë³µì¡ë„ëŠ” <span style="color:#BA6835"> $O(N \log N)$ </span>ì¸ ê²ƒì´ ì¦ëª…ë˜ì—ˆë‹¤.

### Recursive view vs Convolutive view

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%207.png)

- **<U>Recursive view</U>**
    - **ì¥ì **
        1. SSMì˜ Stateì™€ Outputì„ Input sequeceì˜ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤. â†’ <U>ì‹œìŠ¤í…œì˜ ë™ì  ê±°ë™ì„ ì‹œê³„ì—´ì ìœ¼ë¡œ ì§ì ‘ ë¶„ì„í•  ìˆ˜ ìˆë‹¤. </U>
        2. <U>ê¸´ sequenceì—ì„œë„ ì´ˆê¸° ë°ì´í„°ì˜ ì˜í–¥ì„ ë¬´ì‹œí•˜ì§€ ì•Šê²Œ ëœë‹¤.</U> ë”°ë¼ì„œ ì‹œê³„ì—´ì  ë°ì´í„°ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ <span style="color:#BA6835">inductive bias</span>ë¥¼ ì£¼ì…í•  ìˆ˜ ìˆë‹¤.
    - ë‹¨ì 
        1. ìˆœì°¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬í™”(parallelization)ê°€ í˜ë“¤ë‹¤. ë”°ë¼ì„œ <U>í•™ìŠµ ì†ë„ê°€ ëŠë¦¬ë‹¤.</U> (Same with RNN)
        2. ë„ˆë¬´ ê¸´ sequenceë¥¼ í•™ìŠµí•  ë•Œ <U>Vanishing Gradient</U> ë˜ëŠ” <U>Exploding Gradient</U> ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. (Same with RNN)

- **<U>Convolutive view</U>**
    - ì¥ì 
        1. ì£¼íŒŒìˆ˜ í•„í„°ë¥¼ í†µí•´ dataì˜ <span style="color:#BA6835">ì§€ì—­ì ì¸ íŒ¨í„´</span>ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— Input dataì˜ ì§€ì—­ì ì¸ íŠ¹ì§•ì„ ì˜ í¬ì°©í•  ìˆ˜ ìˆë‹¤. â†’ <U>í•´ì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ !</U>
        2. ì—¬ëŸ¬ filterì˜ ë™ì‹œ ì‘ë™ì´ ê°€ëŠ¥, convolutional ê³„ì‚°ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. â†’ <U>ê³„ì‚° íš¨ìœ¨ì´ ë†’ë‹¤ !</U>
    - ë‹¨ì 
        1. Input dataì— ìƒˆë¡œìš´ data pointê°€ ì…ë ¥ë˜ë©´ SSMì˜ ì „ì²´ì ì¸ inputì„ ë§¤ ë²ˆ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ <span style="color:#BA6835">ì˜¨ë¼ì¸ í•™ìŠµ</span> ë˜ëŠ” <span style="color:#BA6835">Autoregressive context</span>ì—ì„œ ì†ë„ê°€ ëŠë¦¬ë‹¤.
        2. ì‹œê³„ì—´ì ì¸ ìƒíƒœ ë³€í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë³¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì‹œìŠ¤í…œì˜ ë‚´ë¶€ ìƒíƒœë¥¼ ì¶”ì í•˜ê¸° ì–´ë µë‹¤.

<aside>
ğŸ’¡ ìƒí™©ì— ë§ê²Œ SSMì˜ ê³„ì‚° ë°©ì‹ì„ ì ì ˆíˆ ì„ íƒí•  í•„ìš”ê°€ ìˆë‹¤.

</aside>

### Linear State-Space Layer(LSSL) Modeling
 
![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%208.png)

- Idea
    - Training : <U>Convolutional view</U>
    - Inference : <U>Recurrent view</U> ë¥¼ ì´ìš©í•˜ì!

- <span style="color:#BA6835">**<U>â€œLinear Time Invariance (ì„ í˜• ì‹œê°„ ë¶ˆë³€ì„±, LTI)â€</U>** â† ë§¤ìš° ì¤‘ìš”</span>
    - LSSLì— ê¸°ë°˜í•œ SSMì˜ parameter $A, B, C$ëŠ” ê° timestepì—ì„œ ì–´ë–¤ tokenì´ ë“¤ì–´ì˜¤ë”ë¼ë„ ë™ì¼í•˜ë‹¤. â†’ ì´ë¥¼ LTIë¼ê³  ë¶€ë¦„
    - **ê° tokenì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ì¬ì¡°ì •í•˜ëŠ” Attention mechanismê³¼ëŠ” ëŒ€ì¡°ì ì„.**
    - SSMì— ì–´ë–¤ sequenceë¥¼ ì œê³µí•˜ë“  ê°„ì— $A, B, C$ì˜ ê°’ì€ ë™ì¼í•˜ë‹¤. ì´ëŠ” ì¦‰ **<U>â€œë‚´ìš© ì¸ì‹ì´ ì—†ëŠ” ì •ì  í‘œí˜„â€</U>**ì„ ê°€ì§€ê³  ìˆë‹¤.

### Importance of $A$ matrix

- SSMì˜ parameter($A, B, C$)ë¥¼ ì‚´í´ë³´ë©´, ê³„ì† Updateë˜ëŠ” parameterëŠ” $A$ì´ë‹¤.
- <U>$A$ matrixì˜ ì˜ë¯¸ëŠ” ë­˜ê¹Œ?</U>
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%209.png)
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2010.png)
    
    â†’ <span style="color:#BA6835">ì´ì „ stateì— ëŒ€í•œ ì •ë³´ë¥¼ í¬ì°©í•˜ì—¬ ìƒˆë¡œìš´ stateë¥¼ êµ¬ì¶•í•œë‹¤. </span>
    
    **<span style="color:#BA6835">ë”°ë¼ì„œ $A$ matrixë¥¼ ì–´ë–»ê²Œ ì •í•˜ëƒì— ë”°ë¼ <U>SSMì´ ì–¼ë§ˆë§Œí¼(ê³¼ê±°~í˜„ì¬ / $0,1,2,â€¦,k$)ì˜ tokenì„ ë°˜ì˜í•  ê²ƒì¸ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.</U></span>**
    
    ê·¸ëŸ¬ë©´, ìš°ë¦¬ëŠ” <U>Long-Contextì— ëŒ€í•´ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ê¸°ì–µí•˜ë„ë¡ $A$ matrixë¥¼ ìƒì„±</U>í•´ì•¼ í•œë‹¤. â†’ HiPPOì˜ ë°œëª… ! 
    
- **HiPPO (High-order Polynomial Projection Operators) - Albert Gu, Tri Dao, et al. (10. 2020)**
    - HiPPO-LegS(Legendre State Space) Matrix
        
        ![img1.daumcdn.png](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/img1.daumcdn.png)
        
    - $n$ê³¼ $k$ì˜ ì˜ë¯¸ëŠ” ë­˜ê¹Œ?
        
        â†’ $n > k$ : everything below the diagonal of matrix $A$
        
        $n = k$ : the diagonal of matrix $A$
        
        $n < k$ : everything above the diagonal of matrix $A$
        
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2011.png)
    
    - <span style="color:#BA6835"><U>Aë¥¼ Learnable parameterê°€ ì•„ë‹ˆë¼, ì¡°ê±´ì— ë”°ë¥¸ ì‹ìœ¼ë¡œ ì§ì ‘ designí•˜ì—¬ ìƒíƒœ í‘œí˜„ ìµœì í™”ë¥¼ ì´ë¤„ëƒˆë‹¤!</U></span>
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2012.png)
    
    - HiPPOë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•í•œ $A$ matrixê°€ $A$ matrixë¥¼ ë¬´ì‘ìœ„ í–‰ë ¬ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë‚˜ì€ ê²ƒì´ Experimentë¥¼ í†µí•´ ì¦ëª…ë¨.
        
        â†’ ì˜¤ë˜ëœ ì‹ í˜¸ë³´ë‹¤ **<U>ìƒˆë¡œìš´ ì‹ í˜¸</U>**ì— ë” ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ë³´ë‹¤ ì •í™•í•˜ê²Œ dataë¥¼ ì¬êµ¬ì„±í•œë‹¤.
        
    
- **Structured State Spaces for Long Sequences(S4) - Albert Gu, et al. (11. 2021) - ICLR 2022**
    - HiPPOì˜ $A$ matrixë¥¼ SSMì— ì ìš©í•˜ì—¬ SSM parameterë¥¼ updateí•œë‹¤.
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2013.png)
    
    - S4ì˜ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œ
        - State Space Model
        - HiPPO for long-range dependancy
        - Discretization for recurrent & Convolutional representations - using Bilinear transform
    - S4ì—ì„œì˜ êµ¬ì²´ì ì¸ A matrix êµ¬ì„±
        - $A$ matrixë¥¼ <span style="color:#BA6835">HiPPO matrixë¡œ initialization</span>
        - $A$ matrixë¥¼ **<U>NPLR(Normal Plus Low-Rank)</U>** êµ¬ì¡°ë¡œ ë³€í™˜ â†’ <U>Diagonalizationì— ê·¼ì‚¬</U> & <U>Low Rank</U> ë³´ì • ìˆ˜í–‰
            - NPLR í˜•íƒœì˜ $A = V \Lambda V^* - PQ^*$ â† **HiPPO matrix**ë¥¼ ë‹¤ë¥´ê²Œ í‘œí˜„í•œ ê²ƒì„.
                - $\Lambda$ : Diagonal matrix
                - $P, Q$ : Low-Rank matrix
                - $V$ : Identity matrix
            - **<U>Woodbury Identity</U>**ë¥¼ í†µí•œ Low-Rank ë³´ì •
                - $(\Lambda - PQ^*)^{-1} = \Lambda^{-1} + \Lambda^{-1}P(I - Q^*\Lambda{-1}P)^{-1}Q^*\Lambda^{-1}$
        - Convolutional ì—°ì‚°ì—ì„œ Cauchy Kernel ì‚¬ìš© â†’ <span style="color:#BA6835">ê³„ì‚° ë³µì¡ë„ë¥¼ ì¤„ì´ê³  $A$ matrixì˜ ì•ˆì •ì„± í–¥ìƒ</span>
    
- **S4D (Structured State Space with Diagonal state matrix) - Albert Gu, et al. (06. 2022)**
    - **<span style="color:#BA6835"><U>DiffuSSMì—ì„œ SSM architectureì˜ backboneìœ¼ë¡œ ì‚¬ìš©</U></span>**
    - S4ì—ì„œì˜ $A$ matrix diagonalizationì„ ë³´ì •í•˜ì—¬ ê°„ë‹¨í™”!
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2014.png)
    
    - Continuous sequenceì˜ Discretizationì—ì„œ <U>ZOH</U>, <U>Bilinear</U> ëª¨ë‘ ì‚¬ìš©
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2015.png)
    
    - HiPPO matrixì—ì„œ <U>Low-Rankì¸ $PQ^*$ë¥¼ ì œê±°í•˜ì—¬ ë‹¨ìˆœí™”</U> â†’ **<U><span style="color:#BA6835">ì™„ë²½í•œ Diagonalization !</span></U>**

- **Mamba - Linear-Time Sequence Modeling with Selective State Spaces (S6) - Albert Gu, et al. (12.2023)**
    - Discretization using <U>ZOH</U>
    - **Selective Mechanism**
        
        ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2016.png)
        
        - Albert Guì˜ ê¸°ì¡´ì˜ SSM ê¸°ë°˜ ëª¨ë¸ì€ $A$ matrixë¥¼ ë¹„ë¡¯í•œ **Parameter updateë¥¼ ê³ ì •ëœ ruleë¡œ ì§„í–‰**í•˜ì˜€ë‹¤. â†’ **<U>input dataì™€ ë…ë¦½ì ìœ¼ë¡œ update ì§„í–‰.</U>**
        - Mambaì—ì„œëŠ” <span style="color:#BA6835">**<U>parameterë¥¼ input dataì— ì˜ì¡´í•˜ì—¬ Selectiveí•˜ê²Œ update ì§„í–‰!</U>**</span>
            
            ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2017.png)
            
        - ê° Timestepì—ì„œì˜ parameter $A, B, C$ë¥¼ $A_t, B_t, C_t$ë¼ê³  í•œë‹¤ë©´
            - $A_t = A + \Delta A(x_t)$
            $B_t = B + \Delta B(x_t)$
            $C_t = C + \Delta C(x_t)$
            
            $\Delta A(x_t), \Delta B(x_t), \Delta C(x_t)$ : ì…ë ¥ $x_t$ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” í•­
            
        - **Selective function**
            - $\sigma_t = \sigma(W_s x_t + b_s)$
            
            $h_t$ : í˜„ì¬ state
            
            $\tilde{h}_t$ : ì´ì „ state
            
            $\odot$ : element-wise multiplication
            
        - **Selective State update** : Selective functionì„ ì´ìš©í•˜ì—¬ state update ì§„í–‰
            - $h_t = \sigma_t \odot h_t + (1 - \sigma_t) \odot \tilde{h}_t$
                
                
                $h_t$ : í˜„ì¬ state
                
                $\tilde{h}_t$ : ì´ì „ state
                
                $\odot$ : element-wise multiplication
                
        - **<span style="color:#BA6835">ê·¸ë˜ì„œ ì–´ë–»ê²Œ Selectí•˜ëŠ”ë°?</span>**
            
            â†’ ê³ í•´ìƒë„(high resolution)ì˜ time sequence dataì—ì„œ <U>ì¤‘ìš”í•œ ì´ë²¤íŠ¸ë‚˜ ë³€í™”(ex. ê°ì²´ì˜ ê²½ê³„, í…ìŠ¤íŠ¸ ë“±)ê°€ ìˆëŠ” timestep</U>ì—ì„œ ZOHì˜ ê°„ê²©ì„ ì‘ê²Œ ì¤Œìœ¼ë¡œì¨ ë” ìì£¼ update, ëœ ì¤‘ìš”í•œ timestepì€ ë¹„êµì  ì ê²Œ update!
            
    
    - **Hardware-aware Parallel Scan Algorithm**
        
        ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2018.png)
        
        - **Kernel Fusion**
            - SSM training ì§„í–‰ ì‹œ, GPU ë‚´ë¶€ì—ì„œ <span style="color:#BA6835">HBM(High Bandwidth Memory)ì´ ì•„ë‹Œ SRAM(Static Random Access Memory, Cache)</span>ì—ì„œ parameter ì €ì¥ ë° ê³„ì‚°ì„ <U>kernelë¡œ ìœµí•©í•˜ì—¬ ì§„í–‰</U>
        - **Recomputation**
            - Forwardpropagationì—ì„œ Backpropagationì— í•„ìš”í•œ intermediate state(Partial derivation value ë“±)ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  Backpropagationì—ì„œ ì¬ê³„ì‚° ì§„í–‰. â†’ <span style="color:#BA6835">Memory spatial complexity ìµœì í™”</span>
    - Overview of Mamba
        
        ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2019.png)
        
    
    <aside>
    â“   Albert GuëŠ” Mamba ì´í›„ì— ê³¼ì—° ì–´ë–¤ ëª¨ë¸ì„ ë§Œë“¤ê³  ìˆì„ê¹Œ?
    
    </aside>
    

## 4.2. DiffuSSM Block

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled.png)

DiffuSSM modelì˜ ì „ì²´ì ì¸ Pipelineì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. Input data(ex. ì´ë¯¸ì§€)ì— ì ì§„ì ìœ¼ë¡œ Noiseë¥¼ ì¶”ê°€í•˜ì—¬ Forward process ì§„í–‰, Input sequence ìƒì‚°
2. Hourglass Dense Layerë¥¼ í†µí•´ Input sequenceë¥¼ down-scaling
3. Bidirectional-SSM Blockì—ì„œ Noiseë¥¼ ì œê±°(ë³µì›)í•˜ëŠ” Backward process ì§„í–‰
4. ë‹¤ì‹œ Hourglass Dense Layerì—ì„œ Noise ì œê±°ëœ sequenceë¥¼ up-scaling
5. Hourglass Fusion Layerì—ì„œ ë³µì›ëœ sequenceì™€ ì›ë˜ original input sequenceë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… output ìƒì‚°

ë§¨ ì²˜ìŒ ë“¤ì–´ì˜¤ëŠ” Input sequenceëŠ” <span style="color:#BA6835">ê¸¸ì´ $J$ì™€ ì°¨ì› $D$ë¥¼ ê°€ì§„ $I$ ($I \in \mathbb{R}^{J \times D}$) </span>ë¡œ ê°€ì •í•œë‹¤.

### Input Sequence Processing

Input sequence $I$ë¥¼ ë°›ì•„ ì••ì¶•(Down-scale)í•˜ê³  í™•ì¥(Up-scale)í•˜ì—¬ ì¤‘ê°„ í‘œí˜„ $U_l$ë¥¼ ìƒì„±í•œë‹¤.

- **Down-scaling**
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2020.png)
    
    Down-scalingì€ ì¼ë°˜ì ìœ¼ë¡œ Average Pooling ë˜ëŠ” Linear transformì„ í†µí•´ ìˆ˜í–‰ëœë‹¤. DiffuSSMì€ <U>Linear transform</U>ì„ ì´ìš©í•˜ì˜€ë‹¤.
    
    <span style="background-color:#BA6835">$U_l = \sigma(W_k^\uparrow \sigma(W^0 I_j))$ </span>
    
    - $I$ : Input sequence ($I \in \mathbb{R}^{J \times D}$**)**
    - $W_0$ : Input sequenceì— ëŒ€í•œ Linear transform matrix
    - $W_k^\uparrow$ :Down-scalingì„ ìœ„í•œ Linear transform matrix
    - $\sigma$ : Activation function
    - **$U_l$** : Down-scaled sequence ($U_l \in \mathbb{R}^{L \times D}$)

- **Bidirectional SSM (using S4D)**
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2021.png)
    
    Hourglass architectureë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„(Down-scaled) ì¤‘ê°„ í‘œí˜„ $U$ë¥¼ inputìœ¼ë¡œ ë°›ì•„ output sequence $Y$ë¥¼ ìƒì„±í•œë‹¤.
    

<span style="background-color:#BA6835"> $Y = \text{Bidirectional-SSM}(U)$ </span>

- $U$ : Down-scaled sequence
- $Y$ : Bidirectional SSMì—ì„œ ìƒì„±ëœ Output sequence ($Y \in \mathbb{R}^{L \times 2D}$)

ì—¬ê¸°ì„œ SSMì˜ backbone modelë¡œ ì•ì„œ ì–¸ê¸‰í•œ S4Dë¥¼ ì´ìš©í•œë‹¤.

S4Dì˜ matrix definitionì„ ê°„ë‹¨íˆ Recapí•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$A_d = (I - \frac{T}{2} A)^{-1} (I + \frac{T}{2} A)$
$B_d = (I - \frac{T}{2} A)^{-1} T B$
$C_d = C$
$D_d = D$

- Forward S4D

$x_f[k+1] = A_d x_f[k] + B_d u[k]$ 
$y_f[k] = C_d x_f[k] + D_d u[k]$ 

- Backward S4D

$x_b[k+1] = A_d x_b[k] + B_d u[k]$
$y_b[k] = C_d x_b[k] + D_d u[k]$

- Output concat

$y[k] = y_f[k] + y_b[k]$

- **Up-scaling**
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2022.png)
    
    Up-scalingì€ Bidirectional-SSMì˜ output sequence $Y$ ë¥¼ ë‹¤ì‹œ ì›ë˜ì˜ ê¸¸ì´ë¡œ í™•ì¥í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì´ë¥¼ í†µí•´ ì €ì°¨ì› sequenceë¥¼ ë‹¤ì‹œ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì›ë˜ì˜ sequence lengthë¡œ ë³µì›í•œë‹¤. 
    
    Up-scalingì€ Linear Transformì„ í†µí•´ ìˆ˜í–‰ëœë‹¤.
    
    <span style="background-color:#BA6835"> $I'_{j, Dm:k:Dm(k+1)} = \sigma(W_k^\downarrow Y_l)$ </span>
    
    - $Y$ : Bidirectional SSMì—ì„œ ìƒì„±ëœ ì¶œë ¥ ì‹œí€€ìŠ¤ ($Y \in \mathbb{R}^{L \times 2D}$)
    - $W^\downarrow_k$ : Up-scalingì„ ìœ„í•œ Linear transform matrix
    - $\sigma$ : Activation function
    - $I'$ : Up-scaled sequence ($I' \in \mathbb{R}^{J \times 2D}$**)**
    

### Output Sequence Processing

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2023.png)

- Hourglass Fusion Layer
    
    Hourglass Fusion Layerì—ì„œ Hourglass Dense Layerì™€ Bidirectional SSMì—ì„œ ì–»ì€ outputì„ ê²°í•©(Element-wise add)í•˜ì—¬ ìµœì¢… outputì„ ì¶œë ¥í•œë‹¤.
    
    <span style="background-color:#BA6835">$O_j = W^3(\sigma(W^2 I'_j) \odot \sigma(W^1 I_j))$</span>
    
    - $I'$ : Up-scaled sequence
    - $I$ : Original Input squence (â‰  input image)
    - $W_1, W_2, W_3$ : Linear tranform matrix
    - $\sigma$ : Activation function
    - $O_j$ : ìµœì¢… Output sequence

# 5. Experiment

## 5.1. Experimental Setup

### Datasets

- ImageNet-1k : 1.28m images, 1k class
- LSUN : Church (126k images), Bedroom(3M images)ì˜ ë‘ category
- ImageNet : 256x256 & 512x512, LSUN : 256*256 í•´ìƒë„ë¡œ ì§„í–‰

### Linear Decoding and Weight Intialization

- Gated SSMì˜ ìµœì¢… ë¸”ë¡ ì´í›„ modelì€ **<U>sequantial image representation</U>**ì„ ì›ë˜ì˜ **<U>original spatial demension</U>**ìœ¼ë¡œ decodingí•˜ì—¬ <span style="color:#BA6835">noise ë° diagonal covarianceì˜ ì˜ˆì¸¡ì„ ì¶œë ¥</span>í•œë‹¤.

### Training Configuration

- DiTì˜ í›ˆë ¨ ì„¤ì •ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ëª¨ë“  modelì— ë™ì¼í•œ ì„¤ì • ì ìš©
- ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì˜ ê°ì‡ ìœ¨(decay)ë¥¼ ì¼ì •í•œ ìƒìˆ˜ë¡œ ì„¤ì •
- Pre-trained VAE Encoderë¥¼ ì‚¬ìš©, í›ˆë ¨ ì¤‘ paremeter ê³ ì •.
- DiffuSSM-XL modelì˜ ê²½ìš° ì•½ 673M parameterë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, Bidirectional-SSM Block(Gated)ì€ 29 layerë¡œ êµ¬ì„± (similar to DiT-XL)
- <U>Computational costë¥¼ ì¤„ì´ê¸° ìœ„í•´</U> **<span style="color:#BA6835">Mixed-precision training</span>**ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì§„í–‰

### Metrics

- Frechet Inception Distance (FID), sFID, Inception Score, Precision/Recall metricì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„± ì„±ëŠ¥ í‰ê°€ ì§„í–‰
- Classifier-free guidanceëŠ” ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ” í•œ ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤.

### Implementation and Hardware

- Implemented all models in **Pytorch**
- Trained them using **NVIDIA A100 GPU 80GB * 8**, with a **global batch size of 256**.

## 5.2. Baselines

- ì´ì „ì˜ ìµœê³  ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì˜€ìŒ. ì—¬ê¸°ì—ëŠ” GAN-style approches, UNet, Latent spaceì—ì„œ ì‘ë™í•˜ëŠ” Transformerê°€ í¬í•¨ëœë‹¤.
- ì£¼ëœ ë¹„êµ ëª©í‘œëŠ” DiffuSSMì˜ ì„±ëŠ¥ì„ ë‹¤ë¥¸ baselineê³¼ ë¹„êµí•˜ëŠ” ê²ƒì´ë‹¤.
- 256*256 í•´ìƒë„ ìˆ˜ì¤€ì˜ ì´ë¯¸ì§€ ìƒì„±ì— ì¤‘ì ì„ ë‘ì—ˆìœ¼ë©°, DDPM Framework ë‚´ì—ì„œ ë¹„êµí•˜ì˜€ë‹¤.

## 5.3. Experimental Results

### Class-Conditional Image Generation

- ë‹¤ë¥¸ SOTA class-conditional generative modelê³¼ ë¹„êµí–ˆì„ ë•Œ FIDì™€ sFIDì—ì„œ ë‹¤ë¥¸ Diffudion modelì„ ëŠ¥ê°€í–ˆìœ¼ë©°, training step ìˆ˜ë¥¼ ì•½ 3ë°° ì¤„ì˜€ë‹¤.
- Total Gflopsì—ì„œ DiTë³´ë‹¤ 20% ê°ì†Œí•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.
- Classifier-free guidanceë¥¼ ì ìš©í–ˆì„ ë•Œ, DiffuSSMì€ ëª¨ë“  DDPM ê¸°ë°˜ ëª¨ë¸ ì¤‘ ìµœê³  sFID ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìœ¼ë©° space distortionì—ì„œ ë” robustí•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤.

### Unconditional Image Generation

- LSUN datasetì—ì„œì˜ ë¹„êµ ê²°ê³¼ - DiffuSSMì€ LDMê³¼ ë¹„ìŠ·í•œ FID ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ì´ëŠ” DiffuSSMì´ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ê³¼ì œì—ì„œ ì ìš© ê°€ëŠ¥í•¨ì„ ë³´ì¸ë‹¤.
- LSUN-Bedroomsì—ì„œëŠ” ì´ í›ˆë ¨ ì˜ˆì‚°ì˜ 25%ë§Œ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ADMì„ ëŠ¥ê°€í•˜ì§€ëŠ” ëª» í–ˆë‹¤.

# 6. Analysis

### Additional Images

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2024.png)

### Model Scaling

- ì„¸ ê°€ì§€ ë‹¤ë¥¸ í¬ê¸°(S, L, XL)ì˜ DiffuSSM modelì„ í›ˆë ¨í•˜ì—¬ ëª¨ë¸ í¬ê¸° í™•ì¥ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì •í•˜ì˜€ë‹¤.
    - S : hidden dimension size D(-S/D) = 384
    - L : hidden dimension size D(-L/D) = 786
    - XL : hidden dimention size D(-XL/D) = 1152
    
    ![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2025.png)
    
- DiT ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê²Œ, ëª¨ë¸ì˜ í¬ê¸°ê°€ í´ ìˆ˜ë¡ FLOPsë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©° DiffuSSMì˜ í¬ê¸°ë¥¼ í™•ì¥í•˜ë©´ í›ˆë ¨ì˜ ëª¨ë“  ë‹¨ê³„ì—ì„œ FIDê°€ í–¥ìƒëœë‹¤.

### Impact of Hourglass

- Latent Spaceì—ì„œ ì••ì¶•ì˜ ì˜í–¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ìƒ˜í”Œë§ ì„¤ì •ìœ¼ë¡œ Model training ì§„í–‰.
- Downsampling ratio $M$ = 2 ë¡œ ì¡°ì ˆí•œ modelê³¼, Patch size $P$ = 2(similar to what DiT has done) ë¡œ ì¡°ì ˆí•œ model ë‘ ê°€ì§€ë¥¼ ì‹¤í—˜í•˜ì—¬ ë¹„êµ.

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2026.png)

### Qualitative Analysis

- DiffuSSMì˜ ëª©ì ì€ <span style="color:#BA6835">**<U>hidden representationì˜ ì••ì¶•ì„ í•˜ì§€ ì•ŠëŠ” ê²ƒ</U>**</span>ì´ë‹¤.
- ì„¸ ëª¨ë¸ ë³€í˜• ëª¨ë‘ ë™ì¼í•œ batch í¬ê¸°ì™€ ë‹¤ë¥¸ hyperparameterë¡œ 400K stepì˜ trainingì„ ì§„í–‰í•œë‹¤.
- Image generating ê³¼ì •ì—ì„œ ëª¨ë‘ ë™ì¼í•œ initial noiseì™€ noise scheduleì„ class label ì „ë°˜ì— ê±¸ì³ ì‚¬ìš©í–ˆë‹¤.

![Untitled](/assets/img/2024-05-18-Diffusion%20Models%20Without%20Attention&SSM/Untitled%2027.png)

- Image patchingì˜ ê³¼ì •ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ê°™ì€ í›ˆë ¨ ì¡°ê±´ì—ì„œ spatial reconstruction ê³¼ì •ì˜ Robustnessë¥¼ í–¥ìƒì‹œì¼°ë‹¤.
- Computational costë¥¼ ëŒ€í­ ì¤„ì´ë©´ì„œë„ uncompressed modelê³¼ ê²¬ì¤„ ìˆ˜ ìˆì„ ë§Œí¼ì˜ image qualityë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤.

# 7. Conclusion

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Attention mechanismì„ ì´ìš©í•˜ì§€ ì•Šê³  State Space Modelì„ ì´ìš©í•˜ì—¬ Diffusion modelì„ êµ¬ì„±í•˜ëŠ” architectureì¸ DiffuSSMì„ ì†Œê°œí•˜ì˜€ë‹¤. 

DiffuSSMì„ í†µí•˜ì—¬ Representation compressionì„ ì´ìš©í•˜ì§€ ì•Šê³  long-ranged hidden stateë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

ë” ì ì€ FLOpsë¥¼ ì´ìš©í•˜ëŠ” DiffuSSM architectureë¥¼ í†µí•´, 256x256 í•´ìƒë„ì˜ ì´ë¯¸ì§€ì—ì„œ ê¸°ì¡´ì˜ DiT ëª¨ë¸ë³´ë‹¤ ë” ì ì€ trainingì„ í†µí•´ ë” ë‚˜ì€ í¼í¬ë¨¼ìŠ¤ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. ë˜í•œ ë” ë†’ì€ í•´ìƒë„ì—ì„œë„ 256x256ì˜ í•´ìƒë„ì™€ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.

í•˜ì§€ë§Œ ì´ ì‘ì—…ì—ëŠ” ëª‡ ê°€ì§€ ì œí•œ ì‚¬í•­ì´ ë‚¨ì•„ ìˆë‹¤.

1. (un)conditional image generationì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìœ¼ë©° full Text-to-Imageì˜ ì ‘ê·¼ë²•ì€ ë‹¤ë£¨ì§€ ì•ŠëŠ”ë‹¤.
2. Masked image trainingê³¼ ê°™ì´ ìµœê·¼ì— ë°œí‘œëœ ì ‘ê·¼ ë°©ì‹ì´ ëª¨ë¸ì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ì 

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ DiffuSSMì´ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ì—ì„œ Diffusion modelì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ ë” ë‚˜ì€ model architectureë¥¼ ì œê³µí•œë‹¤ëŠ” ê²ƒì€ ì—¬ì „íˆ ìœ íš¨í•˜ë‹¤.

DiffuSSMì€ attention mechanismì˜ ë³‘ëª© í˜„ìƒì„ ì œê±°í•¨ìœ¼ë¡œì¨ hi-fi audio, video, 3D modelingê³¼ ê°™ì€ long-rangeì˜ diffusionì´ í•„ìš”í•œ ë¶„ì•¼ë“¤ì—ì„œ ì‘ìš© ê°€ëŠ¥ì„±ì„ ë„“í˜€ ì£¼ì—ˆë‹¤.